## Анализ поведения покупателей в онлайн-магазине

**Тема:** анализ поведения покупателей в онлайн-магазине и факторов, влияющих на покупки.  
**Датасет:** Shopping Behavior Dataset (≈3900 наблюдений, 18 признаков).  
**Цель:** сегментировать покупателей по паттернам поведения и построить модель, прогнозирующую подписку на рассылку / сервис (поле `Subscription Status`).

### Основные задачи проекта
- **EDA и очистка данных**: обзор структуры датасета, проверка типов, пропусков, дубликатов, распределений признаков и корреляций (ноутбук `01_eda.ipynb`).
- **Кластеризация покупателей**: KMeans + DBSCAN, поиск оптимальных параметров и оценка качества через **Silhouette Score** (ноутбук `02_clustering.ipynb` и скрипт `main.py`).
- **Интерпретация сегментов**: построение профилей кластеров по числовым и категориальным признакам (ноутбук `03_cluster_profiles.ipynb`).
- **Классификация (Subscription Status)**: обучение моделей **RandomForest** и **SVM**, выбор лучшей по метрике **ROC-AUC** и сохранение итоговой модели (ноутбук `04_classification.ipynb` и скрипт `main.py`).
- **Визуализация**: сохранение ключевых графиков (распределения, кластеры в PCA-пространстве, ROC-кривые) в `results/figures`.

### Архитектура проекта
- **`main.py`** – основной скрипт, который:
  - загружает сырые данные из `data/raw/shopping_behavior.csv`;
  - строит матрицу признаков через общий препроцессинг;
  - подбирает `k` для KMeans (PCA(15) + перебор `k` с 2 до 15);
  - перебирает параметры для DBSCAN по только числовым признакам;
  - обучает модели классификации (RandomForest и SVM) для таргета `Subscription Status` (целевой признак выносится из фичей);
  - сохраняет лучшую модель в `models/best_model.joblib`, метрики в `results/reports/metrics_main.json` и графики в `results/figures`.

- **`src/`** – модульный код пайплайна:
  - `data_preprocessing.py` – единый препроцессинг:
    - числовые признаки: `SimpleImputer(strategy="median")` + `StandardScaler`;
    - категориальные признаки: `SimpleImputer(strategy="most_frequent")` + `OneHotEncoder(handle_unknown="ignore")`;
    - реализованы функции `build_preprocess_pipeline` и `fit_transform_preprocess` для удобного переиспользования в скриптах и ноутбуках.
  - остальные модули (`clustering.py`, `modeling.py`, `evaluation.py`, `visualization.py`, `utils.py`) зарезервированы под вынос логики из `main.py` (сейчас основная логика сосредоточена в `main.py` и ноутбуках).

- **`notebooks/`** – исследовательские ноутбуки:
  - `01_eda.ipynb` – первичный анализ данных, распределения и корреляции;
  - `02_clustering.ipynb` – экспериментальный подбор параметров кластеризации;
  - `03_cluster_profiles.ipynb` – расшифровка и анализ полученных сегментов;
  - `04_classification.ipynb` – эксперименты с моделями классификации и важностью признаков.

- **`data/`**:
  - `data/raw/` – сырые данные (`shopping_behavior.csv`, не коммитятся в репозиторий);
  - `data/processed/` – возможные сохранённые промежуточные версии датасетов.

- **`results/`**:
  - `results/figures/` – все сохранённые графики (силуэт для KMeans, визуализация кластеров через PCA, ROC-кривые, корреляции и т.п.);
  - `results/reports/` – json с метриками обучения (`metrics_main.json` и др.).

- **`models/`**:
  - `best_model.joblib` – сериализованная лучшая модель классификации (RandomForest или SVM, в зависимости от ROC-AUC).

### Логика пайплайна (высокоуровневое описание)
- **1. Загрузка данных**
  - CSV-файл `shopping_behavior.csv` читается из `data/raw/`.
  - В EDA-ноутбуке проверяется отсутствие пропусков и дубликатов, типы признаков и базовая статистика.

- **2. Препроцессинг признаков**
  - Используется общий сквозной пайплайн на базе `ColumnTransformer` + `Pipeline` из `scikit-learn`.
  - Числовые признаки обрабатываются медианным заполнением пропусков и нормировкой `StandardScaler`.
  - Категориальные признаки кодируются через `OneHotEncoder` с игнорированием неизвестных категорий.
  - Для задач кластеризации в `fit_transform_preprocess` можно использовать все признаки (включая `Subscription Status` как категориальный) – это оправдано, так как цель сегментации описательная.

- **3. Кластеризация (сегментация покупателей)**
  - **KMeans**:
    - сначала признаки преобразуются и понижаются размерностью с помощью **PCA(15)**;
    - далее перебираются значения `k` от 2 до 15;
    - для каждого `k` считается средний **Silhouette Score**, выбирается лучший `k`;
    - строится и сохраняется график `k` vs Silhouette (`kmeans_pca15_silhouette_by_k.png`), а также scatter-проекция первых двух главных компонент с раскраской по кластерам.
  - **DBSCAN** (на числовых признаках):
    - берутся только числовые колонки: `Age`, `Purchase Amount (USD)`, `Review Rating`, `Previous Purchases`;
    - выполняется нормализация (импьютер + скейлер);
    - перебирается сетка по `eps` и `min_samples`, оценивается количество кластеров, число шумовых точек и Silhouette Score (без шума);
    - лучшая конфигурация логируется, визуализируется в 2D через PCA и сохраняется.

- **4. Классификация (прогноз Subscription Status)**
  - Целевой признак формируется функцией `make_target_subscription` в `main.py`:
    - `"Yes" → 1`, `"No" → 0`, при других значениях выбрасывается ошибка с подсказкой.
  - Из входных признаков **обязательно удаляется** колонка `Subscription Status`, чтобы избежать утечки таргета.
  - Данные разбиваются на train/test с `test_size=0.2` и стратификацией по таргету.
  - Обучаются две модели:
    - **RandomForestClassifier** с 300 деревьями и `class_weight="balanced"`;
    - **SVM (RBF)** с `C=2.0`, `probability=True` и `class_weight="balanced"`.
  - Для обеих моделей считается **ROC-AUC** на тесте, выбирается лучшая, сохраняется:
    - модель → `models/best_model.joblib`;
    - метрики и служебная информация → `results/reports/metrics_main.json`;
    - ROC-кривая лучшей модели → `results/figures/best_roc_curve_main.png`.

### Как запустить проект
- **Требования:**
  - Python 3.10+;
  - установленный `pip`.

- **Шаги установки:**
  1. Клонировать или скопировать репозиторий.
  2. (Рекомендуется) создать виртуальное окружение:

```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux / macOS
```

  3. Установить зависимости:

```bash
pip install -r requirements.txt
```

  4. Скачать датасет **Shopping Behavior Dataset** (например, с Kaggle) и сохранить файл как:

```text
data/raw/shopping_behavior.csv
```

- **Запуск основного пайплайна:**

```bash
python main.py
```

После выполнения появятся:
- сохранённая модель в `models/best_model.joblib`;
- файлы метрик в `results/reports/metrics_main.json`;
- графики в `results/figures/`.

### Как интерпретировать результаты
- **Метрики в `metrics_main.json`:**
  - `kmeans_best_k`, `kmeans_best_silhouette` – итоговое количество кластеров и их качество по Silhouette;
  - `dbscan_clusters`, `dbscan_noise_points`, `dbscan_silhouette_no_noise` – характеристики кластеров DBSCAN;
  - `rf_auc`, `svm_auc`, `best_model`, `best_auc` – качество моделей классификации и победитель.
- **Графики в `results/figures`:**
  - силует для KMeans по разным `k`;
  - визуализация кластеров на первых двух главных компонентах;
  - ROC-кривая лучшей модели, heatmap корреляций и другие вспомогательные визуализации.
- **Профили кластеров:** ноутбук `03_cluster_profiles.ipynb` показывает, какие признаки отличают сегменты (возраст, средние суммы покупок, активность и т.д.), что можно напрямую использовать в маркетинговых гипотезах.

### Идеи для дальнейшего развития (архитектура и код)
- **Вынос логики из `main.py` в модули `src/`:**
  - оформить отдельные функции/классы для кластеризации, перебора гиперпараметров, обучения классификаторов, сохранения артефактов и логирования;
  - это упростит повторное использование и тестирование, а также сделает код чище.
- **Конфигурация через файлы настроек:**
  - держать пути к данным/результатам, параметры моделей (диапазон `k`, сетку для DBSCAN, гиперпараметры RF/SVM) в `config.yaml` или `config.json`;
  - тогда менять эксперименты можно будет без правок кода.
- **Логирование вместо `print`:**
  - использовать модуль `logging` с уровнями (`INFO`, `DEBUG`, `WARNING`) и сохранением логов в файл.
- **Расширение оценки моделей:**
  - добавить `classification_report`, confusion matrix и PR-кривые для основной модели прямо в пайплайн;
  - сохранить дополнительные отчёты в `results/reports/`.
- **Тестирование и стабильность:**
  - добавить простые unit-тесты для `data_preprocessing.py` и создания таргета;
  - проверить устойчивость к неожиданным значениям в `Subscription Status` и другим аномалиям в данных.

Все эти улучшения можно внедрить постепенно, не меняя базовую логику проекта и не ломая текущий сценарий запуска через `main.py`.

